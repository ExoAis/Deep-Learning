{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97192a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "808d4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_calculator(orig_dims, layers, layer_dims):\n",
    "    di = {}\n",
    "    orig_dim, _, orig_depth = orig_dims\n",
    "    di[str(0)] = orig_depth\n",
    "    for i in range(len(layers)):\n",
    "        if i == 0:\n",
    "            hparameters = layer_dims[i]\n",
    "            f, s, _, p = hparameters\n",
    "            di[str(i + 1)] = math.floor(((orig_dim + 2 * p - f) / (s)) + 1)\n",
    "        elif layers[i] == 'Pool':\n",
    "            hparameters = layer_dims[i]\n",
    "            f, s, _ = hparameters\n",
    "            di[str(i + 1)] = math.floor(((di[str(i)] - f) / (s)) + 1)\n",
    "        elif layers[i] == 'Conv':\n",
    "            hparameters = layer_dims[i]\n",
    "            f, s, _, p = hparameters\n",
    "            di[str(i + 1)] = math.floor(((di[str(i)] + 2 * p - f) / (s)) + 1)\n",
    "        elif layers[i] == 'Flatten':\n",
    "            hparameters = layer_dims[i]\n",
    "            f_no = hparameters\n",
    "            di[str(i + 1)] = di[str(i)] * di[str(i)] * f_no\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5ca6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(i, layer_dims, activations, initialization, Adam, n_prev = 0):\n",
    "    if n_prev == 0:\n",
    "        if initialization == True:\n",
    "            if activations[i - 1] == 'linear' or activations[i - 1] == 'relu':\n",
    "                W = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1 / layer_dims[i - 1])\n",
    "            else:\n",
    "                W = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "        else:\n",
    "            W = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        if Adam == True:\n",
    "            Vdw = np.zeros((layer_dims[i], layer_dims[i - 1]))\n",
    "            Vdb = np.zeros((layer_dims[i], 1))\n",
    "            Sdw = np.zeros((layer_dims[i], layer_dims[i - 1]))\n",
    "            Sdb = np.zeros((layer_dims[i], 1))\n",
    "            Vcdw = np.zeros((layer_dims[i], layer_dims[i - 1]))\n",
    "            Vcdb = np.zeros((layer_dims[i], 1))\n",
    "            Scdw = np.zeros((layer_dims[i], layer_dims[i - 1]))\n",
    "            Scdb = np.zeros((layer_dims[i], 1))\n",
    "        b = np.zeros((layer_dims[i], 1))\n",
    "    else:\n",
    "        if initialization == True:\n",
    "            if activations[i - 1] == 'linear' or activations[i - 1] == 'relu':\n",
    "                W = np.random.randn(layer_dims[i], n_prev) * np.sqrt(1 / n_prev)\n",
    "            else:\n",
    "                W = np.random.randn(layer_dims[i], n_prev) * np.sqrt(2 / n_prev)\n",
    "        else:\n",
    "            W = np.random.randn(layer_dims[i], n_prev) * 0.01\n",
    "        if Adam == True:\n",
    "            Vdw = np.zeros((layer_dims[i], n_prev))\n",
    "            Vdb = np.zeros((layer_dims[i], 1))\n",
    "            Sdw = np.zeros((layer_dims[i], n_prev))\n",
    "            Sdb = np.zeros((layer_dims[i], 1))\n",
    "            Vcdw = np.zeros((layer_dims[i], n_prev))\n",
    "            Vcdb = np.zeros((layer_dims[i], 1))\n",
    "            Scdw = np.zeros((layer_dims[i], n_prev))\n",
    "            Scdb = np.zeros((layer_dims[i], 1))\n",
    "        b = np.zeros((layer_dims[i], 1))\n",
    "    return W, Vdw, Vdb, Sdw, Sdb, Vcdw, Vcdb, Scdw, Scdb, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a24238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_conv(layers, hparameters, hparameters_prev, orig_depth = -1):\n",
    "    if orig_depth == -1:\n",
    "        f, s, f_no, p = hparameters\n",
    "        _, _, f_no_prev, _ = hparameters_prev\n",
    "        W = np.random.randn(f, f, f_no_prev, f_no)\n",
    "        b = np.zeros((1, 1, 1, f_no))\n",
    "        if Adam == True:\n",
    "            Vdw = np.zeros((f, f, f_no_prev, f_no))\n",
    "            Vdb = np.zeros((1, 1, 1, f_no))\n",
    "            Sdw = np.zeros((f, f, f_no_prev, f_no))\n",
    "            Sdb = np.zeros((1, 1, 1, f_no))\n",
    "            Vcdw = np.zeros((f, f, f_no_prev, f_no))\n",
    "            Vcdb = np.zeros((1, 1, 1, f_no))\n",
    "            Scdw = np.zeros((f, f, f_no_prev, f_no))\n",
    "            Scdb = np.zeros((1, 1, 1, f_no))\n",
    "    else:\n",
    "        f, s, f_no, p = hparameters\n",
    "        W = np.random.randn(f, f, orig_depth, f_no)\n",
    "        b = np.zeros((1, 1, 1, f_no))\n",
    "        if Adam == True:\n",
    "            Vdw = np.zeros((f, f, orig_depth, f_no))\n",
    "            Vdb = np.zeros((1, 1, 1, f_no))\n",
    "            Sdw = np.zeros((f, f, orig_depth, f_no))\n",
    "            Sdb = np.zeros((1, 1, 1, f_no))\n",
    "            Vcdw = np.zeros((f, f, orig_depth, f_no))\n",
    "            Vcdb = np.zeros((1, 1, 1, f_no))\n",
    "            Scdw = np.zeros((f, f, orig_depth, f_no))\n",
    "            Scdb = np.zeros((1, 1, 1, f_no))\n",
    "    return W, Vdw, Vdb, Sdw, Sdb, Vcdw, Vcdb, Scdw, Scdb, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b673a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(orig_dims, layers, layer_dims, activations, initialization, Adam):\n",
    "    parameters = {}\n",
    "    di = dimension_calculator(orig_dims, layers, layer_dims)\n",
    "    orig_dim, _, orig_depth = orig_dims\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        if layers[i - 1] == 'Dense' and layers[i - 2] == 'Flatten':\n",
    "            n_prev = di[str(i - 1)]\n",
    "            parameters['W' + str(i)], parameters['Vdw' + str(i)], parameters['Vdb' + str(i)], parameters['Sdw' + str(i)], parameters['Sdb' + str(i)], parameters['Vcdw' + str(i)],parameters['Vcdb' + str(i)], parameters['Scdw' + str(i)], parameters['Scdb' + str(i)], parameters['b' + str(i)] = initialize_parameters_deep(i - 1, layer_dims, activations, initialization, Adam, n_prev)\n",
    "        elif layers[i - 1] == 'Dense':\n",
    "            parameters['W' + str(i)], parameters['Vdw' + str(i)], parameters['Vdb' + str(i)], parameters['Sdw' + str(i)], parameters['Sdb' + str(i)], parameters['Vcdw' + str(i)],parameters['Vcdb' + str(i)], parameters['Scdw' + str(i)], parameters['Scdb' + str(i)], parameters['b' + str(i)] = initialize_parameters_deep(i - 1, layer_dims, activations, initialization, Adam)\n",
    "        elif layers[i - 1] == 'Conv':\n",
    "            if i == 1:\n",
    "                hparameters = layer_dims[i - 1]\n",
    "                parameters['W' + str(i)], parameters['Vdw' + str(i)], parameters['Vdb' + str(i)], parameters['Sdw' + str(i)], parameters['Sdb' + str(i)], parameters['Vcdw' + str(i)],parameters['Vcdb' + str(i)], parameters['Scdw' + str(i)], parameters['Scdb' + str(i)], parameters['b' + str(i)] = initialize_parameters_conv(layers, hparameters, [], orig_depth)\n",
    "            elif layers[i - 2] != 'Pool':\n",
    "                hparameters = layer_dims[i - 1]\n",
    "                hparameters_prev = layer_dims[i - 2]\n",
    "                parameters['W' + str(i)], parameters['Vdw' + str(i)], parameters['Vdb' + str(i)], parameters['Sdw' + str(i)], parameters['Sdb' + str(i)], parameters['Vcdw' + str(i)],parameters['Vcdb' + str(i)], parameters['Scdw' + str(i)], parameters['Scdb' + str(i)], parameters['b' + str(i)] = initialize_parameters_conv(layers, hparameters, hparameters_prev)\n",
    "            elif layers[i - 2] == 'Pool':\n",
    "                hparameters = layer_dims[i - 1]\n",
    "                hparameters_prev = layer_dims[i - 3]\n",
    "                parameters['W' + str(i)], parameters['Vdw' + str(i)], parameters['Vdb' + str(i)], parameters['Sdw' + str(i)], parameters['Sdb' + str(i)], parameters['Vcdw' + str(i)],parameters['Vcdb' + str(i)], parameters['Scdw' + str(i)], parameters['Scdb' + str(i)], parameters['b' + str(i)] = initialize_parameters_conv(layers, hparameters, hparameters_prev)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3617177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode = 'constant', constant_values = (0, 0))\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f5eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b087b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    temp = np.exp(Z)\n",
    "    A = temp / np.sum(temp, axis = 0)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430b061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = (1 / (1 + np.exp(-Z)))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "417f1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f951d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8097148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(Z):\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1141d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_slice_convolution(a_slice_prev, W, b):\n",
    "    s = a_slice_prev * W\n",
    "    Z = np.sum(s)\n",
    "    Z += np.float64(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34068fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward(A_prev, W, b, activation, keep_prob):\n",
    "    Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A, activation_cache = tanh(Z)\n",
    "    elif activation == 'leakyrelu':\n",
    "        A, activation_cache = leakyrelu(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation == 'linear':\n",
    "        if keep_prob != 1:\n",
    "            D = np.random.rand(Z.shape[0], Z.shape[1])\n",
    "            D = (D < keep_prob).astype(int)\n",
    "            Z = D * Z\n",
    "            Z /= keep_prob\n",
    "            A_prev, W, b = linear_cache\n",
    "            linear_cache = (D, A_prev, W, b)\n",
    "        cache = linear_cache\n",
    "        return Z, cache\n",
    "    if keep_prob != 1:\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = D * A\n",
    "        A /= keep_prob\n",
    "        A_prev, W, b = linear_cache\n",
    "        linear_cache = (D, A_prev, W, b)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c216cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters, activation):\n",
    "    m, n_H_prev, n_W_prev, n_C = A_prev.shape\n",
    "    f, f, n_C_prev, n_C = W.shape\n",
    "    _, stride, _, pad = hparameters\n",
    "    n_H = math.floor(((n_H_prev + 2 * pad - f) / (stride)) + 1)\n",
    "    n_W = math.floor(((n_W_prev + 2 * pad - f) / (stride)) + 1)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):\n",
    "                    a_prev_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
    "                    weight = W[:,:,:,c]\n",
    "                    bias = b[:,:,:,c]\n",
    "                    Z[i, h, w, c] = single_slice_convolution(a_prev_slice, weight, bias)\n",
    "                    if activation == 'relu':\n",
    "                        A[i, h, w, c], cach = relu(Z[i, h, w, c])\n",
    "                    elif activation == 'sigmoid':\n",
    "                        A[i, h, w, c], cach = sigmoid(Z[i, h, w, c])\n",
    "                    elif activation == 'linear':\n",
    "                        A[i, h, w, c] = Z[i, h, w, c]\n",
    "                    elif activation == 'tanh':\n",
    "                        A[i, h, w, c], cach = tanh(Z[i, h, w, c])\n",
    "    cache = (A_prev, W, b, hparameters, cach)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19624e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, keep_prob):\n",
    "    m, n_H_prev, n_W_prev, n_C = A_prev.shape\n",
    "    f, stride, mode = hparameters\n",
    "    n_H = math.floor(((n_H_prev - f) / (stride)) + 1)\n",
    "    n_W = math.floor(((n_W_prev - f) / (stride)) + 1)\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    if mode == 'max':\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == 'average':\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    D = np.random.rand(A.shape[0], A.shape[1], A.shape[2], A.shape[3])\n",
    "    D = (D < keep_prob).astype(int)\n",
    "    A = D * A\n",
    "    A /= keep_prob\n",
    "    cache = (A_prev, hparameters, D)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af47e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_forward(A_prev):\n",
    "    cache = (A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3])\n",
    "    A = A_prev.reshape(A_prev.shape[0], A_prev.shape[1] * A_prev.shape[2] * A_prev.shape[3])\n",
    "    return A.T, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cb82f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_forward(X, layers, layer_dims, parameters, activations, keep_prob):\n",
    "    A = X\n",
    "    caches = []\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        A_prev = A\n",
    "        if i < len(layers):\n",
    "            if layers[i - 1] == 'Dense':\n",
    "                A, cache = single_layer_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activations[i - 1], keep_prob[i - 1])\n",
    "            elif layers[i - 1] == 'Conv':\n",
    "                hparameters = layer_dims[i - 1]\n",
    "                A, cache = conv_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], hparameters, activations[i - 1])\n",
    "            elif layers[i - 1] == 'Pool':\n",
    "                hparameters = layer_dims[i - 1]\n",
    "                A, cache = pool_forward(A_prev, hparameters, keep_prob[i - 1])\n",
    "            elif layers[i - 1] == 'Flatten':\n",
    "                A, cache = flatten_forward(A_prev)\n",
    "        else:\n",
    "            A, cache = single_layer_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activations[i - 1], 1)\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20bd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, activation):\n",
    "    m = Y.shape[1]\n",
    "    if activation == 'linear':\n",
    "        cost = (1 / (2 * m)) * (np.sum((AL - Y) ** 2))\n",
    "    elif activation == 'softmax':\n",
    "        cost = (- 1 / m) * (np.sum(Y * np.log(AL + 1e-8)))\n",
    "    else:\n",
    "        cost = (- 1 / m) * (np.sum(np.multiply(Y, np.log(AL + 1e-8)) + np.multiply(1 - Y, np.log(1 - AL + 1e-8))))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12045af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, activation, keep_prob):\n",
    "    if keep_prob != 1:\n",
    "        D, A_prev, W, b = cache\n",
    "    else:\n",
    "        A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1078df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative(dA):\n",
    "    dZ = dA\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    s = (1 / (1 + np.exp(-Z)))\n",
    "    dZ = dA * np.multiply(s, 1 - s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b5206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3fbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0.01\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    A = np.tanh(Z)\n",
    "    dZ = 1 - np.power(A, 2)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c0c099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward(dA, caches, activation, keep_prob):\n",
    "    if activation == 'linear':\n",
    "        linear_cache = caches\n",
    "        if keep_prob != 1:\n",
    "            D, c, v, b = linear_cache\n",
    "            dA = dA * D\n",
    "            dA /= keep_prob\n",
    "            linear_cache = (D, c, v, b)\n",
    "        dZ = linear_derivative(dA)\n",
    "    else:\n",
    "        linear_cache, activation_cache = caches\n",
    "        if keep_prob != 1:\n",
    "            D, c, v, b = linear_cache\n",
    "            dA = dA * D\n",
    "            dA /= keep_prob\n",
    "            linear_cache = (D, c, v, b)\n",
    "        if activation == 'sigmoid':\n",
    "            dZ = sigmoid_derivative(dA, activation_cache)\n",
    "        elif activation == 'softmax':\n",
    "            dZ = dA\n",
    "        elif activation == 'tanh':\n",
    "            dZ = tanh_derivative(dA, activation_cache)\n",
    "        elif activation == 'relu':\n",
    "            dZ = relu_derivative(dA, activation_cache)\n",
    "        elif activation == 'leakyrelu':\n",
    "            dZ = leakyrelu_derivative(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c297b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dA, cache, activation):\n",
    "    A_prev, W, b, hparameters, cach = cache\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_derivative(dA, cach)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_derivative(dA, cach)\n",
    "    elif activation == 'tanh':\n",
    "        dZ = tanh_derivative(dA, cach)\n",
    "    elif activation == 'linear':\n",
    "        dZ = linear_derivative(dA)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    f, f, n_C_prev, n_C = W.shape\n",
    "    _, stride, _, pad = hparameters\n",
    "    m, n_H, n_W, n_C = dZ.shape\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "        dA_prev[i,:,:,:] = da_prev_pad[pad:-pad, pad:-pad,:]\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "589298fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(X):\n",
    "    mask = (X == np.max(X))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e95623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dZ, shape):\n",
    "    n_H, n_W = shape\n",
    "    average = dZ / (n_H * n_W)\n",
    "    a = np.zeros(shape) + average\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d09f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, keep_prob):\n",
    "    A_prev, hparameters, D = cache\n",
    "    dA = dA * D\n",
    "    dA = dA / keep_prob\n",
    "    f, stride, mode = hparameters\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    if mode == 'max':\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                    elif mode == 'average':\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82a3fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_backward(dA, cache):\n",
    "    m, n_H, n_W, n_C = cache\n",
    "    dA = dA.reshape(m, n_H, n_W, n_C)\n",
    "    return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58a93cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_backward(X, Y, A, layers, layer_dims, caches, activations, keep_prob):\n",
    "    grads = {}\n",
    "    count = 1;\n",
    "    if activations[len(activations) - 1] == 'linear' or activations[len(activations) - 1] == 'softmax':\n",
    "        dA = A - Y\n",
    "    else: \n",
    "        dA = - (np.divide(Y, A + 1e-8) - np.divide(1 - Y, 1 - A + 1e-8))\n",
    "    grads['dA' + str(len(layer_dims) - 1)] = dA\n",
    "    for i in range(len(layers) - 1, -1, -1):\n",
    "        if i < len(layers) - 1:\n",
    "            if layers[i] == 'Dense':\n",
    "                grads['dA' + str(i - 1)], grads['dW' + str(i)], grads['db' + str(i)] = single_layer_backward(grads['dA' + str(i)], caches[len(caches) - count], activations[len(activations) - count], keep_prob[i - 1])\n",
    "            elif layers[i] == 'Conv':\n",
    "                grads['dA' + str(i - 1)], grads['dW' + str(i)], grads['db' + str(i)] = conv_backward(grads['dA' + str(i)], caches[len(caches) - count], activations[len(activations) - count])\n",
    "            elif layers[i] == 'Pool':\n",
    "                grads['dA' + str(i - 1)] = pool_backward(grads['dA' + str(i)], caches[len(caches) - count], keep_prob[i - 1])\n",
    "            elif layers[i] == 'Flatten':\n",
    "                grads['dA' + str(i - 1)] = flatten_backward(grads['dA' + str(i)], caches[len(caches) - count])\n",
    "        else:\n",
    "            grads['dA' + str(i - 1)], grads['dW' + str(i)], grads['db' + str(i)] = single_layer_backward(grads['dA' + str(i)], caches[len(caches) - count], activations[len(activations) - count], 1)\n",
    "        count += 1\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af01aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, layers, layer_dims, grads, learning_rate, t, beta_1, beta_2, epsilon, Adam, activations):\n",
    "    for i in range(0, len(layers)):\n",
    "        if layers[i] != 'Pool' and layers[i] != 'Flatten':\n",
    "            if Adam == True:\n",
    "                parameters['Vdw' + str(i + 1)] = (beta_1 * parameters['Vdw' + str(i + 1)]) + ((1 - beta_1) * grads['dW' + str(i)])\n",
    "                parameters['Vdb' + str(i + 1)] = (beta_1 * parameters['Vdb' + str(i + 1)]) + ((1 - beta_1) * grads['db' + str(i)])\n",
    "                parameters['Sdw' + str(i + 1)] = (beta_2 * parameters['Sdw' + str(i + 1)]) + ((1 - beta_2) * np.square(grads['dW' + str(i)]))\n",
    "                parameters['Sdb' + str(i + 1)] = (beta_2 * parameters['Sdb' + str(i + 1)]) + ((1 - beta_2) * np.square(grads['db' + str(i)]))\n",
    "                parameters['Vcdw' + str(i + 1)] = (parameters['Vdw' + str(i + 1)]) / (1 - np.power(beta_1, t))\n",
    "                parameters['Vcdb' + str(i + 1)] = (parameters['Vdb' + str(i + 1)]) / (1 - np.power(beta_1, t))\n",
    "                parameters['Scdw' + str(i + 1)] = (parameters['Sdw' + str(i + 1)]) / (1 - np.power(beta_2, t))\n",
    "                parameters['Scdb' + str(i + 1)] = (parameters['Sdb' + str(i + 1)]) / (1 - np.power(beta_2, t))\n",
    "                parameters['W' + str(i + 1)] = parameters['W' + str(i + 1)] - learning_rate * (np.divide(parameters['Vcdw' + str(i + 1)], np.sqrt(parameters['Scdw' + str(i + 1)]) + epsilon)) \n",
    "                parameters['b' + str(i + 1)] = parameters['b' + str(i + 1)] - learning_rate * (np.divide(parameters['Vcdb' + str(i + 1)], np.sqrt(parameters['Scdb' + str(i + 1)]) + epsilon))\n",
    "            else:\n",
    "                parameters['W' + str(i + 1)] = parameters['W' + str(i + 1)] - learning_rate * grads['dW' + str(i)] \n",
    "                parameters['b' + str(i + 1)] = parameters['b' + str(i + 1)] - learning_rate * grads['db' + str(i)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf7f2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(costs, learning_rate):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iterations (Per Five)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b26a3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(A):\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0][i] < 0.5:\n",
    "            A[0][i] = 0\n",
    "        else:\n",
    "            A[0][i] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb700add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1SA(A, Y):\n",
    "    prec = 0\n",
    "    recall = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for i in range(Y.shape[1]):\n",
    "        if A[0][i] == Y[0][i] and A[0][i] == 1:\n",
    "            TP += 1\n",
    "        elif A[0][i] != Y[0][i] and A[0][i] == 1:\n",
    "            FP += 1\n",
    "        elif A[0][i] != Y[0][i] and A[0][i] == 0:\n",
    "            FN += 1\n",
    "        elif A[0][i] == Y[0][i] and A[0][i] == 0:\n",
    "            TN += 1\n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1Score = (2 * prec * recall) / (prec + recall)\n",
    "    accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
    "    return F1Score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69dcfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_prediction(X, Y, layers, layer_dims, parameters, activations):\n",
    "    keep_prob = []\n",
    "    for i in range(len(activations)):\n",
    "        keep_prob.append(1)\n",
    "    A, _ = n_layer_forward(X, layers, layer_dims, parameters, activations, keep_prob)\n",
    "    if activations[len(activations) - 1] == 'linear':\n",
    "        return A\n",
    "    elif activations[len(activations) - 1] != 'linear' and layer_dims[len(layer_dims) - 1] > 1:\n",
    "        A = multi_class(A)\n",
    "        Y = multi_class(Y)\n",
    "        Accuracy = predict_multiclass(A, Y)\n",
    "        print('The accuracy of the model on Test Set is', Accuracy, '%')\n",
    "    else:\n",
    "        A = predict(A)\n",
    "        F1Score, Accuracy = F1SA(A, Y)\n",
    "        print('F1Score and Accuracy of the model on the Test Set is respectively', F1Score * 100, '% and', Accuracy * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "885b8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class(AL):\n",
    "    A = np.zeros((1, AL.shape[1]))\n",
    "    for i in range(AL.shape[1]):\n",
    "        A[0][i] = np.argmax(AL[:,i])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1855c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiclass(A, Y):\n",
    "    count = 0\n",
    "    for i in range(Y.shape[1]):\n",
    "        if A[0][i] == Y[0][i]:\n",
    "            count += 1\n",
    "    return (count / Y.shape[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6239701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, epochs, time_interval, orig_dims, layers, layer_dims, activations, learning_rate_i, decay_rate, keep_prob, initialization, Adam, trained, params = {}, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8):\n",
    "    if trained == True:\n",
    "        parameters = params\n",
    "    else:\n",
    "        parameters = initialize_parameters(orig_dims, layers, layer_dims, activations, initialization, Adam)\n",
    "    costs = []\n",
    "    for i in range(epochs):\n",
    "        learning_rate = (1 / (1 + (decay_rate * math.floor((i + 1) / (time_interval))))) * learning_rate_i\n",
    "        A, caches = n_layer_forward(X, layers, layer_dims, parameters, activations, keep_prob)\n",
    "        grads = n_layer_backward(X, Y, A, layers, layer_dims, caches, activations, keep_prob)\n",
    "        parameters = gradient_descent(parameters, layers, layer_dims, grads, learning_rate, i + 1, beta_1, beta_2, epsilon, Adam, activations)\n",
    "        if i % 5 == 0 or i == epochs - 1:\n",
    "            cost = compute_cost(A, Y, activations[len(activations) - 1])\n",
    "            print('Cost after' , i, 'epochs', 'is', cost)\n",
    "            costs.append(cost)\n",
    "    plot(costs, learning_rate)\n",
    "    A, caches = n_layer_forward(X, layers, layer_dims, parameters, activations, keep_prob)\n",
    "    if activations[len(activations) - 1] != 'linear' and layer_dims[len(layer_dims) - 1] > 1:\n",
    "        A = multi_class(A)\n",
    "        Y = multi_class(Y)\n",
    "        print(A[0], Y[0])\n",
    "        Accuracy = predict_multiclass(A, Y)\n",
    "        print('The accuracy of the model on Training Set is', Accuracy, '%')\n",
    "    elif activations[len(activations) - 1] != 'linear':\n",
    "        A = predict(A)\n",
    "        F1Score, Accuracy = F1SA(A, Y)\n",
    "        print('F1Score and Accuracy of the model on the Training Set is respectively', F1Score * 100, '% and', Accuracy * 100, '%')\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
