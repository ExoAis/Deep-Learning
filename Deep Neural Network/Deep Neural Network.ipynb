{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49bdae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3802717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, activations, initialization):\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        if initialization == True:\n",
    "            if activations[i - 1] == 'linear' or activations[i - 1] == 'relu':\n",
    "                params['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1 / layer_dims[i - 1])\n",
    "            else:\n",
    "                params['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(2 / layer_dims[i - 1])\n",
    "        else:\n",
    "            params['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n",
    "        params['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bd2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e7f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = (1 / (1 + np.exp(-Z)))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2128d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7c04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd57c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(Z):\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f545fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward(A_prev, W, b, activation, keep_prob):\n",
    "    Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A, activation_cache = tanh(Z)\n",
    "    elif activation == 'leakyrelu':\n",
    "        A, activation_cache = leakyrelu(Z)\n",
    "    elif activation == 'linear':\n",
    "        if keep_prob != 1:\n",
    "            D = np.random.rand(Z.shape[0], Z.shape[1])\n",
    "            D = (D < keep_prob).astype(int)\n",
    "            Z = D * Z\n",
    "            Z /= keep_prob\n",
    "            A_prev, W, b = linear_cache\n",
    "            linear_cache = (D, A_prev, W, b)\n",
    "        cache = linear_cache\n",
    "        return Z, cache\n",
    "    if keep_prob != 1:\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = D * A\n",
    "        A /= keep_prob\n",
    "        A_prev, W, b = linear_cache\n",
    "        linear_cache = (D, A_prev, W, b)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24720e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_forward(X, layer_dims, parameters, activations, keep_prob):\n",
    "    A = X\n",
    "    caches = []\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        A_prev = A\n",
    "        if i < len(layer_dims) - 1:\n",
    "            A, cache = single_layer_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activations[i - 1], keep_prob[i - 1])\n",
    "        else:\n",
    "            A, cache = single_layer_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activations[i - 1], 1)\n",
    "        caches.append(cache)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456cada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, activation):\n",
    "    m = Y.shape[1]\n",
    "    if activation == 'linear':\n",
    "        cost = (1 / (2 * m)) * (np.sum((AL - Y) ** 2))\n",
    "    else:\n",
    "        cost = (- 1 / m) * (np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL))))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "438bc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, activation, keep_prob):\n",
    "    if keep_prob != 1:\n",
    "        D, A_prev, W, b = cache\n",
    "    else:\n",
    "        A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503bb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_derivative(dA):\n",
    "    dZ = dA\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f97a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    s = (1 / (1 + np.exp(-Z)))\n",
    "    dZ = dA * np.multiply(s, 1 - s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0de88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "469f15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0.01\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b96f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(dA, cache):\n",
    "    Z = cache\n",
    "    A = np.tanh(Z)\n",
    "    dZ = 1 - np.power(A, 2)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08a19f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward(dA, caches, activation, keep_prob):\n",
    "    if activation == 'linear':\n",
    "        linear_cache = caches\n",
    "        if keep_prob != 1:\n",
    "            D, c, v, b = linear_cache\n",
    "            dA = dA * D\n",
    "            dA /= keep_prob\n",
    "            linear_cache = (D, c, v, b)\n",
    "        dZ = linear_derivative(dA)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "    else:\n",
    "        linear_cache, activation_cache = caches\n",
    "        if keep_prob != 1:\n",
    "            D, c, v, b = linear_cache\n",
    "            dA = dA * D\n",
    "            dA /= keep_prob\n",
    "            linear_cache = (D, c, v, b)\n",
    "        if activation == 'sigmoid':\n",
    "            dZ = sigmoid_derivative(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "        elif activation == 'tanh':\n",
    "            dZ = tanh_derivative(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "        elif activation == 'relu':\n",
    "            dZ = relu_derivative(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "        elif activation == 'leakyrelu':\n",
    "            dZ = leakyrelu_derivative(dA, activation_cache)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache, activation, keep_prob)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5d51cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_layer_backward(X, Y, A, layer_dims, caches, activations, keep_prob):\n",
    "    grads = {}\n",
    "    count = 0;\n",
    "    if activations[len(activations) - 1] == 'linear':\n",
    "        dA = A - Y\n",
    "    else: \n",
    "        dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "    grads['dA' + str(len(layer_dims) - 1)] = dA\n",
    "    for i in range(len(layer_dims) - 1, 0, -1):\n",
    "        if i < len(layer_dims) - 1:\n",
    "            grads['dA' + str(i - 1)], grads['dW' + str(i)], grads['db' + str(i)] = single_layer_backward(grads['dA' + str(i)], caches[len(caches) - 1 - count], activations[len(activations) - 1 - count], keep_prob[i - 1])\n",
    "        else:\n",
    "            grads['dA' + str(i - 1)], grads['dW' + str(i)], grads['db' + str(i)] = single_layer_backward(grads['dA' + str(i)], caches[len(caches) - 1 - count], activations[len(activations) - 1 - count], 1)\n",
    "        count += 1\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70c860f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, layer_dims, grads, learning_rate):\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters['W' + str(i)] = parameters['W' + str(i)] - learning_rate * grads['dW' + str(i)]\n",
    "        parameters['b' + str(i)] = parameters['b' + str(i)] - learning_rate * grads['db' + str(i)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b559d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(costs, learning_rate):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iterations (Per Five)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49c0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(A):\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0][i] < 0.5:\n",
    "            A[0][i] = 0\n",
    "        else:\n",
    "            A[0][i] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fbd95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1SA(A, Y):\n",
    "    prec = 0\n",
    "    recall = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for i in range(Y.shape[1]):\n",
    "        if A[0][i] == Y[0][i] and A[0][i] == 1:\n",
    "            TP += 1\n",
    "        elif A[0][i] != Y[0][i] and A[0][i] == 1:\n",
    "            FP += 1\n",
    "        elif A[0][i] != Y[0][i] and A[0][i] == 0:\n",
    "            FN += 1\n",
    "        elif A[0][i] == Y[0][i] and A[0][i] == 0:\n",
    "            TN += 1\n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1Score = (2 * prec * recall) / (prec + recall)\n",
    "    accuracy = (TP + TN) / (TP + FN + TN + FP)\n",
    "    return F1Score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a035c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_prediction(X, Y, layer_dims, parameters, activations):\n",
    "    A = X\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        A_prev = A\n",
    "        A, cache = single_layer_forward(A_prev, parameters['W' + str(i)], parameters['b' + str(i)], activations[i - 1], 1)\n",
    "    if activations[len(activations) - 1] == 'linear':\n",
    "        return A\n",
    "    elif activations[len(activations) - 1] != 'linear' and layer_dims[len(layer_dims) - 1] > 1:\n",
    "        A = multi_class(A)\n",
    "        Y = multi_class(Y)\n",
    "        Accuracy = predict_multiclass(A, Y)\n",
    "        print('The accuracy of the model on Test Set is', Accuracy, '%')\n",
    "    else:\n",
    "        A = predict(A)\n",
    "        F1Score, Accuracy = F1SA(A, Y)\n",
    "        print('F1Score and Accuracy of the model on the Test Set is respectively', F1Score * 100, '% and', Accuracy * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540d4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class(AL):\n",
    "    A = np.zeros((1, AL.shape[1]))\n",
    "    for i in range(AL.shape[1]):\n",
    "        A[0][i] = np.argmax(AL[:,i])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71347e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiclass(A, Y):\n",
    "    count = 0\n",
    "    for i in range(Y.shape[1]):\n",
    "        if A[0][i] == Y[0][i]:\n",
    "            count += 1\n",
    "    return (count / Y.shape[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8baabb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, iterations, layer_dims, activations, learning_rate, keep_prob, initialization):\n",
    "    parameters = initialize_parameters(layer_dims, activations, initialization)\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        A, caches = n_layer_forward(X, layer_dims, parameters, activations, keep_prob)\n",
    "        grads = n_layer_backward(X, Y, A, layer_dims, caches, activations, keep_prob)\n",
    "        parameters = gradient_descent(parameters, layer_dims, grads, learning_rate)\n",
    "        if i % 5 == 0 or i == iterations - 1:\n",
    "            cost = compute_cost(A, Y, activations[len(activations) - 1])\n",
    "            print('Cost at iteration', i, 'is', cost)\n",
    "            costs.append(cost)\n",
    "    plot(costs, learning_rate)\n",
    "    if activations[len(activations) - 1] != 'linear' and layer_dims[len(layer_dims) - 1] > 1:\n",
    "        A = multi_class(A)\n",
    "        Y = multi_class(Y)\n",
    "        print(A[0], Y[0])\n",
    "        Accuracy = predict_multiclass(A, Y)\n",
    "        print('The accuracy of the model on Training Set is', Accuracy, '%')\n",
    "    elif activations[len(activations) - 1] != 'linear':\n",
    "        A = predict(A)\n",
    "        F1Score, Accuracy = F1SA(A, Y)\n",
    "        print('F1Score and Accuracy of the model on the Training Set is respectively', F1Score * 100, '% and', Accuracy * 100, '%')\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
