{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2c97ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810422fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis:, ] / depth\n",
    "    angle_rads = (positions / (10000 ** depths))\n",
    "    pos_enc = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis = -1)\n",
    "    return tf.cast(pos_enc, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d24c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = positional_encoding(2048, d_model)\n",
    "        \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self, X):\n",
    "        length = tf.shape(X)[1]\n",
    "        X = self.embedding(X)\n",
    "        X *= tf.math.sqrt(tf.cast(self.d_model, dtype = tf.float32))\n",
    "        return X + self.pos_enc[tf.newaxis, :length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea60617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57edbc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, X):\n",
    "        self_attention = self.mha(query = X, value = X, key = X)\n",
    "        out = self.add([X, self_attention])\n",
    "        return self.layer_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5df7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, X, Context):\n",
    "        cross_attention = self.mha(query = X, key = Context, value = Context)\n",
    "        out = self.add([X, cross_attention])\n",
    "        return self.layer_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d14b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(BaseAttention):\n",
    "    def call(self, X):\n",
    "        masked_attention = self.mha(query = X, key = X, value = X, use_causal_mask = True)\n",
    "        out = self.add([X, masked_attention])\n",
    "        return self.layer_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a6888f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, d_model, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    def call(self, X):\n",
    "        X_ = self.seq(X)\n",
    "        out = self.add([X, X_])\n",
    "        return self.layer_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57f22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, units, d_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.gsa = GlobalSelfAttention(num_heads = heads, key_dim = d_model, dropout = dropout_rate)\n",
    "        self.feed_forward = FeedForward(units, d_model, dropout_rate = 0.1)\n",
    "    def call(self, X):\n",
    "        out = self.gsa(X)\n",
    "        return self.feed_forward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2acf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, vocab_size, heads, units, d_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.pos_emb = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.enc_seq = [EncoderLayer(heads, units, d_model, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, X):\n",
    "        out = self.pos_emb(X)\n",
    "        out = self.dropout(out)\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.enc_seq[i](out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f29e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, heads, units, d_model, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.csa = CasualSelfAttention(num_heads = heads, key_dim = d_model, dropout = dropout_rate)\n",
    "        self.ca = CrossAttention(num_heads = heads, key_dim = d_model, dropout = dropout_rate)\n",
    "        self.feed_forward = FeedForward(units, d_model, dropout_rate = 0.1)\n",
    "    def call(self, X, Context):\n",
    "        out_1 = self.csa(X)\n",
    "        out_2 = self.ca(out_1, Context)\n",
    "        return self.feed_forward(out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59acab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, vocab_size, heads, units, d_model, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.pos_emb = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.dec_seq = [DecoderLayer(heads, units, d_model, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, X, Context):\n",
    "        out = self.pos_emb(X)\n",
    "        out = self.dropout(out)\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.dec_seq[i](out, Context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce8e0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, vocab_size, target_vocab_size, heads, units, d_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(num_layers, vocab_size, heads, units, d_model, dropout_rate)\n",
    "        self.dec = Decoder(num_layers, vocab_size, heads, units, d_model, dropout_rate)\n",
    "        self.final = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs):\n",
    "        Context, X = inputs\n",
    "        out_1 = self.enc(Context)\n",
    "        out_2 = self.dec(X, out_1)\n",
    "        out_3 = self.final(out_2)\n",
    "        try:\n",
    "            del out_3._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return out_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e42d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb63ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98,\n",
    "                                     epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec4b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    mask = label != 0\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer = Transformer(\n",
    "    NUM_LAYERS, VOCAB_SIZE, VOCAB_SIZE, NUM_HEADS, UNITS, D_MODEL, DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa89346",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer.compile(loss = masked_loss, optimizer = optimizer, metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer.fit(# dataset\n",
    "    , epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b743521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizer, transformer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "    def __call__(self, sentence, START_TOKEN, END_TOKEN, MAX_LENGTH):\n",
    "        sentence = preprocess_sentence(sentence) #Preprocess the input sentence\n",
    "        sentence = START_TOKEN + self.tokenizer.encode(sentence) + END_TOKEN #Add start and end tokens\n",
    "        sentence = tf.convert_to_tensor(sentence, dtype = tf.int32) \n",
    "        sentence = sentence[np.newaxis, :]\n",
    "        output_array = tf.TensorArray(dtype = tf.int64, size = 0, dynamic_size = True)\n",
    "        output_array = output_array.write(0, START_TOKEN)\n",
    "        for i in tf.range(MAX_LENGTH):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            prediction = self.transformer([sentence, output], training = False)\n",
    "            pred_index = prediction[:, -1:, :]\n",
    "            pred_id = tf.argmax(pred_index, axis = -1)\n",
    "            output_array.write(i + 1, pred_id[0])\n",
    "            if pred_id == END_TOKEN:\n",
    "                break\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        output = output.numpy()\n",
    "        output = output.reshape(output.shape[1], )\n",
    "        output = output[1: len(output) - 1]\n",
    "        output = tokenizer.decode(output) #Detokenize the output\n",
    "        output_array.close()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa531b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator = Translator(tokenizer, Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85411204",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Translator('I am fine. What about you?', START_TOKEN, END_TOKEN, MAX_LENGTH)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
